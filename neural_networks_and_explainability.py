# -*- coding: utf-8 -*-
"""Neural networks and explainability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x77-9ymWO2gfI29HJFcBd0bqyR-ciBq7
"""

import os
import json
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import joblib
import optuna
import shap

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

from statsmodels.tsa.statespace.sarimax import SARIMAX


def rmse(a, b):
    return np.sqrt(mean_squared_error(a, b))


def ensure_datetime(df, date_col):
    df[date_col] = pd.to_datetime(df[date_col])
    df = df.sort_values(date_col).reset_index(drop=True)
    df = df.set_index(date_col)
    if df.index.has_duplicates:
        df = df.groupby(df.index).mean()
    return df


cfg = {
    "data_path": "AEP_hourly.csv",
    "target_col": "AEP_MW",
    "date_col": "Datetime",
    "seed": 42,
    "synthetic_weather": True,
    "train_test_split": 0.2,
    "window_sizes": [24, 48],
    "horizons": [1, 24],
    "optuna": {"trials": 12},
    "training": {"epochs": 30, "batch_size": 64},
    "results_dir": "results"
}

np.random.seed(cfg["seed"])
tf.random.set_seed(cfg["seed"])

os.makedirs(cfg["results_dir"], exist_ok=True)
os.makedirs(os.path.join(cfg["results_dir"], "figures"), exist_ok=True)
os.makedirs(os.path.join(cfg["results_dir"], "models"), exist_ok=True)


df = pd.read_csv(cfg["data_path"])
df = ensure_datetime(df, cfg["date_col"])
df = df.asfreq("H")
df[cfg["target_col"]] = df[cfg["target_col"]].interpolate(method="time")


if cfg["synthetic_weather"]:
    n = len(df)
    daily = 8 * np.sin(2 * np.pi * (df.index.hour) / 24)
    doy = df.index.dayofyear.values
    yearly = 10 * np.sin(2 * np.pi * doy / 365)
    noise = np.random.normal(0, 2.0, size=n)
    temp = 15 + yearly + daily + noise
    humidity = 50 + 10 * np.cos(2 * np.pi * (df.index.hour) / 24) + np.random.normal(0, 3, n)
    df["temp_synth"] = temp
    df["hum_synth"] = humidity

df["hour"] = df.index.hour
df["dayofweek"] = df.index.dayofweek
df["month"] = df.index.month
df["is_weekend"] = df["dayofweek"].isin([5, 6]).astype(int)


target = cfg["target_col"]
lags = [24, 48, 168]
for L in lags:
    df[f"lag_{L}"] = df[target].shift(L)

df["rolling_24_mean"] = df[target].shift(1).rolling(window=24, min_periods=1).mean()
df["rolling_168_mean"] = df[target].shift(1).rolling(window=168, min_periods=1).mean()
df = df.dropna()


feature_cols = [c for c in df.columns if c != target]
scaler_X = StandardScaler()
scaler_y = StandardScaler()

X_all = scaler_X.fit_transform(df[feature_cols])
y_all = scaler_y.fit_transform(df[[target]]).flatten()

joblib.dump(scaler_X, os.path.join(cfg["results_dir"], "models", "scaler_X.pkl"))
joblib.dump(scaler_y, os.path.join(cfg["results_dir"], "models", "scaler_y.pkl"))


def create_sequences_multi(X, y, input_len, horizon):
    Xs, ys = [], []
    for start in range(0, len(X) - input_len - horizon + 1):
        Xs.append(X[start:start+input_len])
        ys.append(y[start+input_len:start+input_len+horizon])
    return np.array(Xs), np.array(ys)


n_total = len(df)
n_test = int(n_total * cfg["train_test_split"])
n_train = n_total - n_test


metrics_rows = []


for input_len in cfg["window_sizes"]:
    for horizon in cfg["horizons"]:
        X_seq, y_seq = create_sequences_multi(X_all, y_all, input_len, horizon)
        n_seq = len(X_seq)
        n_test_seq = int(n_seq * cfg["train_test_split"])
        n_train_seq = n_seq - n_test_seq

        X_train = X_seq[:n_train_seq]
        X_test = X_seq[n_train_seq:]
        y_train = y_seq[:n_train_seq]
        y_test = y_seq[n_train_seq:]


        def build_and_train(trial):
            n_units = trial.suggest_categorical("n_units", [32, 64, 128])
            n_layers = trial.suggest_int("n_layers", 1, 2)
            dropout = trial.suggest_float("dropout", 0.0, 0.3)
            lr = trial.suggest_loguniform("lr", 1e-4, 1e-2)

            model = Sequential()
            for i in range(n_layers):
                return_seq = (i < n_layers - 1)
                if i == 0:
                    model.add(LSTM(n_units, return_sequences=return_seq, input_shape=(input_len, X_train.shape[2])))
                else:
                    model.add(LSTM(n_units, return_sequences=return_seq))
                model.add(Dropout(dropout))

            model.add(Dense(horizon))
            opt = tf.keras.optimizers.Adam(learning_rate=lr)
            model.compile(optimizer=opt, loss="mse", metrics=["mae"])

            es = EarlyStopping(patience=4, restore_best_weights=True)
            model.fit(X_train, y_train, validation_split=0.1, epochs=15, batch_size=cfg["training"]["batch_size"], callbacks=[es], verbose=0)

            pred = model.predict(X_test)
            y_true = scaler_y.inverse_transform(y_test[:, 0].reshape(-1, 1))[:, 0]
            y_pred = scaler_y.inverse_transform(pred[:, 0].reshape(-1, 1))[:, 0]
            return rmse(y_true, y_pred)


        study = optuna.create_study(direction="minimize")
        study.optimize(build_and_train, n_trials=cfg["optuna"]["trials"], show_progress_bar=False)

        best_params = study.best_params

        n_units = best_params["n_units"]
        n_layers = best_params["n_layers"]
        dropout = best_params["dropout"]
        lr = best_params["lr"]

        model = Sequential()
        for i in range(n_layers):
            return_seq = (i < n_layers - 1)
            if i == 0:
                model.add(LSTM(n_units, return_sequences=return_seq, input_shape=(input_len, X_train.shape[2])))
            else:
                model.add(LSTM(n_units, return_sequences=return_seq))
            model.add(Dropout(dropout))

        model.add(Dense(horizon))
        opt = tf.keras.optimizers.Adam(learning_rate=lr)
        model.compile(optimizer=opt, loss="mse", metrics=["mae"])

        model_path = os.path.join(cfg["results_dir"], "models", f"lstm_win{input_len}_h{horizon}.h5")
        mc = ModelCheckpoint(model_path, save_best_only=True, monitor="val_loss")
        es = EarlyStopping(patience=6, restore_best_weights=True)

        history = model.fit(
            X_train, y_train,
            validation_split=0.1,
            epochs=cfg["training"]["epochs"],
            batch_size=cfg["training"]["batch_size"],
            callbacks=[es, mc],
            verbose=0
        )

        y_pred = model.predict(X_test)
        y_pred_inv = scaler_y.inverse_transform(y_pred[:, 0].reshape(-1, 1))[:, 0]
        y_true_inv = scaler_y.inverse_transform(y_test[:, 0].reshape(-1, 1))[:, 0]

        cur_rmse = rmse(y_true_inv, y_pred_inv)
        cur_mae = mean_absolute_error(y_true_inv, y_pred_inv)

        metrics_rows.append({
            "window": input_len,
            "horizon": horizon,
            "rmse": float(cur_rmse),
            "mae": float(cur_mae),
            "best_params": best_params
        })


        plt.figure(figsize=(12, 3))
        plt.plot(y_true_inv[-300:], label="actual")
        plt.plot(y_pred_inv[-300:], label="pred")
        plt.legend()
        plt.title(f"Forecast (Window={input_len}, Horizon={horizon})")
        plt.savefig(os.path.join(cfg["results_dir"], "figures", f"forecast_win{input_len}_h{horizon}.png"))
        plt.close()


        try:
            background = X_train[:100]
            test_sample = X_test[:20]
            explainer = shap.DeepExplainer(model, background)
            shap_values = explainer.shap_values(test_sample)
            sv = shap_values[0]
            mean_abs = np.mean(np.abs(sv), axis=(0, 1))
            idx = np.argsort(mean_abs)[::-1][:10]
            plt.figure(figsize=(7, 4))
            sns.barplot(x=mean_abs[idx], y=np.array(feature_cols)[idx])
            plt.tight_layout()
            plt.savefig(os.path.join(cfg["results_dir"], "figures", f"shap_win{input_len}_h{horizon}.png"))
            plt.close()
        except:
            pass


train_series = df[target].iloc[:n_train]
test_series = df[target].iloc[n_train:]
exog = df[feature_cols]
exog_train = exog.iloc[:n_train]
exog_test = exog.iloc[n_train:]

sar_order = (1,1,1)
sar_seasonal = (1,1,1,24)

try:
    sar = SARIMAX(train_series, order=sar_order, seasonal_order=sar_seasonal, exog=exog_train)
    sar_res = sar.fit(disp=False)
    sar_pred = sar_res.get_forecast(steps=len(test_series), exog=exog_test).predicted_mean

    sar_rmse = rmse(test_series.values, sar_pred.values)
    sar_mae = mean_absolute_error(test_series.values, sar_pred.values)

    pd.DataFrame([{
        "model": "SARIMAX",
        "rmse": sar_rmse,
        "mae": sar_mae
    }]).to_csv(os.path.join(cfg["results_dir"], "sarimax_metrics.csv"), index=False)

except:
    pass


pd.DataFrame(metrics_rows).to_csv(os.path.join(cfg["results_dir"], "metrics.csv"), index=False)

with open(os.path.join(cfg["results_dir"], "final_config.json"), "w") as f:
    json.dump(cfg, f, indent=2)


with open(os.path.join(cfg["results_dir"], "report_summary.txt"), "w") as f:
    f.write("AEP Forecasting Project Completed.\n")
    f.write("See metrics.csv, sarimax_metrics.csv, and figures folder.\n")


print("Project completed. All results saved in 'results' folder.")